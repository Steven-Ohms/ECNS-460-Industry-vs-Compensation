---
title: "Labor Economics Exploration"
author: "Hans Petersen & Steven Ohms"
date: "2023-11-01"
output: html_document
---

This project aims to assess relationships and trends between industry productivity and worker compensation for the year 2021. Productivity measures the efficiency of industries in generating economic output, while worker compensation reflects the wages received by employees. The findings of this project may offer insights between industry productivity and worker compensation.

Analyzing the relationship between industry productivity and worker compensation could provide useful information for policymakers, labor organizations, and businesses. The findings of this project can allow for better informed economic decisions to be made, specifically in relation to labor market policies, wage negotiations, and economic development initiatives. Furthermore, the prospect of having access to this much data, can not only provide useful insights, but also seems very exciting.


Prepare environment
```{r, warning=FALSE, message=FALSE}
rm(list = ls()) #prepare environment

require("ggplot2")
library(ggplot2)
require("forecast") 
library(forecast)
require("readxl")
library(readxl)
require("tidyverse")
library(tidyverse)
```



Load Data
```{r}
#Requires working directory to be .../ECNS-460-Industry-vs-Compensation
industry_data = read_excel("rawData/Industry.xlsx")

head(industry_data)

labor_productivity_data = read_excel("rawData/labor-productivity-detailed-industries.xlsx", sheet = "MachineReadable")

head(labor_productivity_data)
```


Now that we imported the data, the first task is to clean it.
The first thing of note is that there are no null values in the labor_productivity dataset, and only null values in collumns we will probably not even use in the industry dataset
The next major noticable thing before we go through with the data checklist, and the inner join, is matching the date ranges
The industry data only contains variables from 2021, and the labor productivity contains data from 1988-2022. Therefore we need to get rid of all of the data in the labor productivity dataset that arent from 2021
One more thing is that the Industry data contains a period attribute (1st quarter, 2nd quarter, annual, etc). In order to make things match up, we need to only account for annual data, as that is what is being recorded when we filter for years in the labor productivity dataset


```{r}
labor_productivity_2021_data = labor_productivity_data %>% #filters 2021 data
  filter(Year == 2021)

industry_data_annual = industry_data %>% #filters Annual data for the industry data
  filter(Period == "Annual")

industry_data_annual<-industry_data_annual|>
  filter("NAICS" != 221) #remove a duplicate of utilities.
```


Now we need to inner join the datasets. Thankfully industry codes are standardized.
however in the Industry dataset, the industry code attribute contains the number and the name, while the attribute in the labor productivity
dataset only has the code. Therefore we need to separate the number and name in the industry dataset

```{r}

industry_data_separated = industry_data_annual %>%  #Separates the industry data so that NAICS is its own attribute
  separate( 'Industry Code & Title', c("NAICS", "industry"), " -")
```


The next thing we need to do is unite the labor productivity data. This is because the dataset is weird, for example there are multiple different mining 21 rows. The only difference in each row is the measurement, unit, and corresponding value found in each row. However this would make sense to add all of these measurement/unit combination into a new attribute (Because there are multiple, and a lot of repeats of the same measurement/unit combo, and only one corresponding value)


```{r}
united_labor_productivity_data = labor_productivity_2021_data %>% 
  unite("measureUnits", Measure, Units, sep = "_") %>%   #Creates a new attribute measureUnits, which is the combination of measure and units
  pivot_wider(names_from = measureUnits, values_from = "Value") #Creates a new attribute, with the name of measureUnits, and the values from Value
```


Now that we have all of the data cleaned, we should be ready to inner join. We will be joining the NAICS code. One thing to note is that there are multiple repeated NAICS codes. This is because there is different ownership associated with each NAICS in the Industry dataset. Ownership in our dataset is a categorical variable, based on whether it is Private, Federal Gov, Local Gov, or State Gov owned This is fine to still inner merge on this, as we will filter the data by ownership later on in our analysis.


```{r}
#combine all ownership types for each industry
industry_aggregated <- industry_data_separated |>
  group_by(NAICS) |>
  summarise(#sum split categories
    `# of Establishments` = sum(`# of Establishments`),
    `Average Employment` = sum(`Average Employment`),
    `Total Wages` = sum(`Total Wages`)
  )
#split off table of static columns to join to the aggregated data
joinTable<- industry_data_separated|>
  select(c("ID","Area", "Period", "NAICS", "industry"))
joinTable<-distinct(joinTable, NAICS, .keep_all = T)

#recombine 
industry_aggregated<-left_join(industry_aggregated, joinTable, by = "NAICS")
```



```{r}
#join labor productivity information to industry data
industry_labor_data = inner_join(industry_aggregated, united_labor_productivity_data, by = "NAICS")

#calculate average salary in each industry assuming a 40 hour work week every week
industry_labor_data$`Average Salary` <- 52*40*industry_labor_data$`Total Wages`/(industry_labor_data$`Hours worked_Millions of hours`*1000000)

cleaned_industry_labor_data<-industry_labor_data #rename dataframe

#calculate labor productivity
cleaned_industry_labor_data$laborProductivity<-cleaned_industry_labor_data$`Sectoral output_Millions of current dollars`/cleaned_industry_labor_data$`Hours worked_Millions of hours`
```


```{r}
#calculate MPL

#calculate the hours worked and sectoral output in 2020 using the given % change
cleaned_industry_labor_data<-cleaned_industry_labor_data|>
  mutate(Labor2020 = `Hours worked_Millions of hours`/ (1+ (`Hours worked_% Change from previous year`/100)))
cleaned_industry_labor_data<-cleaned_industry_labor_data|>
   mutate(Output2020 = `Sectoral output_Millions of current dollars` / (1 + (`Sectoral output_% Change from previous year`/100))) 

#Find difference between 2021 and 2020 hours and output
cleaned_industry_labor_data$laborDif<- cleaned_industry_labor_data$`Hours worked_Millions of hours`-cleaned_industry_labor_data$Labor2020
cleaned_industry_labor_data$ouputDif<- cleaned_industry_labor_data$`Sectoral output_Millions of current dollars` - cleaned_industry_labor_data$Output2020

#calculate MPL
cleaned_industry_labor_data<-cleaned_industry_labor_data|>
  mutate(Marginal_Product = ouputDif / (laborDif))

```

As we can see with this code, the data set's have been joined smoothly. However, due to this join, and previous filters, there becomes a lot of useless attributes. This includes the multiple year attributes (all 2021, no differences), Period (All annual since filtered, no need for duplicates), Area (all US, no differences), and the basis and state names also repeat the same values. With this all applicable steps of the data cleaning checklist have been completed. One final thing to note is that we do still have some missing values in our dataset, specifically when it comes to some of the percent change variables. We are deciding not to remove these rows/columns of data, as we already have enough data points, to where removing the columns entirely could cause us to miss potential connections, and removing the rows would remove way too much of our dataset to be worthwhile. When looking at converting variables to the right data type, we are lucky that everything is already properly assigned to their corresponding data type (strings are strings, numbers are numeric, etc). Because of the repeat NAICS, what was supposed to be our primary key, looking at our data, we now see that our primary key has changed to the ID attribute. Finally looking at all of our percent variables, we are lucky that they are all in the form of whole number percentages (5%), and not .05. It doesn't really matter either way, but it is nice that we don't have to do additional work to make sure all of the percent units are the same.

We are now good to move on to making other inferences based off of our newly found Cleaned_industry_Labor_data


```{r, fig.height=10, fig.width=10}
cleaned_industry_labor_data <- cleaned_industry_labor_data %>% #add column ranking industry by sectoral output
  mutate(outputRank = rank(-`Sectoral output_Millions of current dollars`)) 

#create cleaveland dotplot of Labor Productivity by Labor Industry
fig1<-cleaned_industry_labor_data|>
  arrange(laborProductivity)|>
  ggplot(aes(x = `laborProductivity`, y = reorder(Industry, laborProductivity), color = outputRank)) +
  geom_point()+
  scale_color_gradient(low = "blue", high = "red")+
  labs(
    title = "Labor Productivity by Labor Industry", 
    x = "Labor Productivity, dollars per hour", 
    y = "Labor Industry",
    color = "Sectoral Output Rank")
fig1
ggsave("fig1.png", fig1)

#create Cleveland dotplot of salary by labor industry using the same axis as fig1 to show trend
fig2<-cleaned_industry_labor_data|>
  arrange(laborProductivity)|>
  ggplot(aes(x = `Average Salary`, y = reorder(Industry, laborProductivity), color = outputRank)) +
  geom_point()+
  scale_color_gradient(low = "blue", high = "red")+
  labs(
    title = "Average Salary by Labor Industry", 
    x = "Average Anunal Salary", 
    y = "Labor Industry",
    color = "Sectoral Output Rank")
fig2
ggsave("fig2.png",fig2)

#unused level plot of salary and labor productivity

# cleaned_industry_labor_data|>
#   arrange(laborProductivity)|>
#   ggplot(aes(y = `Average Salary`, x =  laborProductivity)) +
#   geom_point() + geom_smooth(method = "lm")

#Log-Log plot of salary and labor productivity to decluster and improve clarity
fig3<-cleaned_industry_labor_data|>
  arrange(laborProductivity)|>
  ggplot(aes(y = log(`Average Salary`), x =  log(laborProductivity))) +
  geom_point() + geom_smooth(method = "lm", se=F)+
    labs(
    title = "Log- Log Salary Vs. Labor productivity by industry", 
    y = "Log of Average Anunal Salary", 
    x = "Log of Labor Productivity, ")
fig3
ggsave("fig3.png", fig3)

#get slope of line of best fit
summary(lm(cleaned_industry_labor_data$`Average Salary`~cleaned_industry_labor_data$laborProductivity))
summary(lm(cleaned_industry_labor_data$`Average Salary`~cleaned_industry_labor_data$Marginal_Product))

summary(lm(log(cleaned_industry_labor_data$`Average Salary`)~log(cleaned_industry_labor_data$laborProductivity)))

#dataframe used for comparing the most important variables easier
 t <-cleaned_industry_labor_data|>
   select("NAICS","laborProductivity", "industry", "Average Salary", "Hours worked_Millions of hours", "Sectoral output_Millions of current dollars", "Marginal_Product")

```





```{r}
#plot the lack of relationship between MPL and Average Salary
fig4<-ggplot(cleaned_industry_labor_data, aes(y = `Average Salary`, x = Marginal_Product)) +
  geom_point() +
  labs(title = "Marginal Product of Labor to Average Salary", x = "Marginal Product of Labor", y = "Average Salary")
fig4
ggsave("fig4.png", fig4, width = 10, height = 8)

#create a dataset without industries that are outliers in labor productivity and labor
noOutlier<-subset(cleaned_industry_labor_data, cleaned_industry_labor_data$NAICS!= 324 & cleaned_industry_labor_data$NAICS!= 447)

#plot Marginal Product of Labor vs Labor Productivity with two lines of best fit, one that takes outliers into account and one that does not
fig5<-ggplot(cleaned_industry_labor_data, aes(x = laborProductivity, y = Marginal_Product)) +
  geom_point() +geom_smooth(method = "lm")+geom_smooth(data = noOutlier, method = "lm", color = "red")+
  labs(title = "Marginal Product of Labor vs Labor Productivity",  x= "Labor Productivity, dollars per hour", y = "Marginal productof labor, ")
fig5
ggsave("fig5.png",fig5)


```
